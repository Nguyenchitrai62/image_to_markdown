import cv2
import numpy as np
from paddleocr import PaddleOCR
import json
import os
from typing import List, Dict, Tuple, Any
from googletrans import Translator
import time

# Import XY-Cut functions (gi·∫£ s·ª≠ ƒë√£ c√≥)
from xycut import recursive_xy_cut, points_to_bbox

def calculate_iou(box1, box2):
    """
    T√≠nh IoU (Intersection over Union) gi·ªØa hai bounding box
    
    Args:
        box1, box2: [x_min, y_min, x_max, y_max] format
        
    Returns:
        float: IoU value (0-1)
    """
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    
    if x1 >= x2 or y1 >= y2:
        return 0.0
    
    intersection = (x2 - x1) * (y2 - y1)
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union = area1 + area2 - intersection
    
    return intersection / union if union > 0 else 0.0

def calculate_overlap_ratio(box1, box2):
    """
    T√≠nh t·ª∑ l·ªá overlap c·ªßa box nh·ªè h∆°n so v·ªõi box l·ªõn h∆°n
    
    Args:
        box1, box2: [x_min, y_min, x_max, y_max] format
        
    Returns:
        float: Overlap ratio (0-1)
    """
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    
    if x1 >= x2 or y1 >= y2:
        return 0.0
    
    intersection = (x2 - x1) * (y2 - y1)
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    
    # T·ª∑ l·ªá overlap c·ªßa box nh·ªè h∆°n
    min_area = min(area1, area2)
    return intersection / min_area if min_area > 0 else 0.0

def has_intersection(box1, box2):
    """
    Ki·ªÉm tra hai bounding box c√≥ giao nhau kh√¥ng (d√π ch·ªâ 1 pixel)
    
    Args:
        box1, box2: [x_min, y_min, x_max, y_max] format
        
    Returns:
        bool: True n·∫øu c√≥ giao nhau, False n·∫øu kh√¥ng
    """
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    
    # C√≥ giao nhau n·∫øu x1 < x2 v√† y1 < y2
    return x1 < x2 and y1 < y2

def convert_bbox_to_xycut_format(bbox):
    """
    Chuy·ªÉn ƒë·ªïi bbox t·ª´ PaddleOCR format sang [x_min, y_min, x_max, y_max]
    """
    try:
        if isinstance(bbox, list) and len(bbox) == 4:
            if isinstance(bbox[0], list):
                # Format: [[x1,y1], [x2,y2], [x3,y3], [x4,y4]]
                x_coords = [point[0] for point in bbox]
                y_coords = [point[1] for point in bbox]
                result = [min(x_coords), min(y_coords), max(x_coords), max(y_coords)]
            else:
                # Format: [x1, y1, x2, y2]
                result = bbox
        elif hasattr(bbox, 'shape') and len(bbox.shape) == 2:
            # Numpy array shape (4, 2)
            x_coords = bbox[:, 0]
            y_coords = bbox[:, 1]
            result = [np.min(x_coords), np.min(y_coords), np.max(x_coords), np.max(y_coords)]
        else:
            # Fallback - convert to numpy and try again
            bbox_array = np.array(bbox)
            if bbox_array.shape == (4, 2):
                x_coords = bbox_array[:, 0]
                y_coords = bbox_array[:, 1]
                result = [np.min(x_coords), np.min(y_coords), np.max(x_coords), np.max(y_coords)]
            elif len(bbox_array) == 8:
                # Format: [x1,y1,x2,y2,x3,y3,x4,y4]
                result = points_to_bbox(bbox_array)
            else:
                result = [0, 0, 100, 100]
        
        # ƒê·∫£m b·∫£o t·∫•t c·∫£ gi√° tr·ªã l√† integers v√† >= 0
        result = [max(0, int(x)) for x in result]
        return result
        
    except Exception:
        return [0, 0, 100, 100]

def find_overlapping_groups(text_infos):
    """
    T√¨m c√°c nh√≥m text boxes c√≥ ch·ªìng l√™n nhau (b·∫•t k·ª≥ m·ª©c ƒë·ªô n√†o)
    
    Args:
        text_infos: List of dict v·ªõi keys: 'text', 'confidence', 'bbox'
        
    Returns:
        List of lists: M·ªói sublist ch·ª©a indices c·ªßa c√°c box ch·ªìng l√™n nhau
    """
    n = len(text_infos)
    if n == 0:
        return []
    
    # Convert t·∫•t c·∫£ bbox v·ªÅ ƒë·ªãnh d·∫°ng chu·∫©n
    boxes = []
    for info in text_infos:
        bbox = convert_bbox_to_xycut_format(info['bbox'])
        boxes.append(bbox)
    
    # T·∫°o ma tr·∫≠n adjacency
    overlap_matrix = np.zeros((n, n), dtype=bool)
    
    for i in range(n):
        for j in range(i + 1, n):
            # Ch·ªâ c·∫ßn ki·ªÉm tra c√≥ giao nhau kh√¥ng (kh√¥ng c·∫ßn ng∆∞·ª°ng)
            if has_intersection(boxes[i], boxes[j]):
                overlap_matrix[i][j] = True
                overlap_matrix[j][i] = True
    
    # T√¨m c√°c connected components (nh√≥m c√°c box ch·ªìng l√™n nhau)
    visited = [False] * n
    groups = []
    
    def dfs(node, current_group):
        visited[node] = True
        current_group.append(node)
        
        for neighbor in range(n):
            if overlap_matrix[node][neighbor] and not visited[neighbor]:
                dfs(neighbor, current_group)
    
    for i in range(n):
        if not visited[i]:
            current_group = []
            dfs(i, current_group)
            groups.append(current_group)
    
    return groups

def merge_overlapping_boxes(text_infos, groups):
    """
    G·ªôp c√°c box overlap th√†nh box l·ªõn v√† s·∫Øp x·∫øp text b·∫±ng XY-Cut
    
    Args:
        text_infos: List of text info dicts
        groups: List of lists, m·ªói sublist ch·ª©a indices c·ªßa c√°c box c·∫ßn g·ªôp
        
    Returns:
        List of merged text info dicts
    """
    merged_results = []
    
    for group in groups:
        if len(group) == 1:
            # Ch·ªâ c√≥ 1 box, kh√¥ng c·∫ßn g·ªôp
            merged_results.append(text_infos[group[0]])
        else:
            # G·ªôp nhi·ªÅu box
            group_texts = []
            group_confidences = []
            all_bboxes = []
            
            # Collect th√¥ng tin c·ªßa t·∫•t c·∫£ box trong group
            for idx in group:
                info = text_infos[idx]
                group_texts.append({
                    'text': info['text'],
                    'confidence': info['confidence'],
                    'bbox': info['bbox']
                })
                group_confidences.append(info['confidence'])
                all_bboxes.append(convert_bbox_to_xycut_format(info['bbox']))
            
            # T√≠nh bounding box g·ªôp
            all_bboxes = np.array(all_bboxes)
            merged_bbox = [
                int(np.min(all_bboxes[:, 0])),  # x_min
                int(np.min(all_bboxes[:, 1])),  # y_min
                int(np.max(all_bboxes[:, 2])),  # x_max
                int(np.max(all_bboxes[:, 3]))   # y_max
            ]
            
            # S·∫Øp x·∫øp text trong group b·∫±ng XY-Cut
            sorted_group_texts = sort_texts_with_xycut(group_texts)
            
            # G·ªôp text v√† t√≠nh confidence trung b√¨nh
            combined_text = " ".join([t['text'] for t in sorted_group_texts])
            avg_confidence = np.mean(group_confidences)
            
            merged_info = {
                'text': combined_text,
                'confidence': float(avg_confidence),
                'bbox': merged_bbox,
                'is_merged': True,
                'merged_from_count': len(group),
                'original_texts': [t['text'] for t in sorted_group_texts]
            }
            
            merged_results.append(merged_info)
    
    return merged_results

def sort_texts_with_xycut(texts_with_info):
    """
    S·∫Øp x·∫øp text theo th·ª© t·ª± ƒë·ªçc s·ª≠ d·ª•ng XY-Cut algorithm
    """
    if not texts_with_info:
        return []
    
    # Chu·∫©n b·ªã d·ªØ li·ªáu cho XY-Cut
    boxes = []
    indices = list(range(len(texts_with_info)))
    
    for i, text_info in enumerate(texts_with_info):
        bbox = text_info.get('bbox', [[0, 0], [0, 0], [0, 0], [0, 0]])
        converted_bbox = convert_bbox_to_xycut_format(bbox)
        boxes.append(converted_bbox)
    
    boxes = np.array(boxes, dtype=np.int32)
    indices = np.array(indices, dtype=np.int32)
    
    if len(boxes) == 0:
        return []
    
    try:
        # Ch·∫°y XY-Cut ƒë·ªÉ l·∫•y th·ª© t·ª± ƒë·ªçc
        reading_order = []
        recursive_xy_cut(boxes, indices, reading_order)
        
        # S·∫Øp x·∫øp texts theo th·ª© t·ª± ƒë·ªçc
        if reading_order:
            sorted_texts = [texts_with_info[i] for i in reading_order]
        else:
            # Fallback v·ªÅ s·∫Øp x·∫øp ƒë∆°n gi·∫£n
            sorted_texts = sort_texts_by_position_fallback(texts_with_info)
            
    except Exception:
        # Fallback v·ªÅ s·∫Øp x·∫øp ƒë∆°n gi·∫£n
        sorted_texts = sort_texts_by_position_fallback(texts_with_info)
    
    return sorted_texts

def sort_texts_by_position_fallback(texts_with_info):
    """Fallback sorting method"""
    def get_sort_key(text_info):
        bbox = text_info.get('bbox', [[0, 0], [0, 0], [0, 0], [0, 0]])
        try:
            if isinstance(bbox[0], list):
                x_coords = [point[0] for point in bbox]
                y_coords = [point[1] for point in bbox]
                center_x = sum(x_coords) / len(x_coords)
                center_y = sum(y_coords) / len(y_coords)
            else:
                center_x, center_y = bbox[0], bbox[1]
            return (center_y, center_x)
        except:
            return (0, 0)
    
    return sorted(texts_with_info, key=get_sort_key)

def translate_text(text, src_lang="ja", dest_lang="vi", max_retries=3):
    """
    D·ªãch text s·ª≠ d·ª•ng Google Translate v·ªõi retry mechanism
    
    Args:
        text: Text c·∫ßn d·ªãch
        src_lang: Ng√¥n ng·ªØ ngu·ªìn
        dest_lang: Ng√¥n ng·ªØ ƒë√≠ch
        max_retries: S·ªë l·∫ßn th·ª≠ l·∫°i t·ªëi ƒëa
        
    Returns:
        str: Text ƒë√£ d·ªãch
    """
    if not text or not text.strip():
        return text
    
    translator = Translator()
    
    for attempt in range(max_retries):
        try:
            result = translator.translate(text, src=src_lang, dest=dest_lang)
            return result.text
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói d·ªãch l·∫ßn {attempt + 1}: {str(e)}")
            if attempt < max_retries - 1:
                time.sleep(1)  # ƒê·ª£i 1 gi√¢y tr∆∞·ªõc khi th·ª≠ l·∫°i
            else:
                print(f"‚ùå Kh√¥ng th·ªÉ d·ªãch text: '{text}', gi·ªØ nguy√™n text g·ªëc")
                return text
    
    return text

def process_image_to_json(image_input, 
                         src_lang="ja",
                         dest_lang="vi",
                         output_json_path=None,
                         save_debug_image=False):
    """
    Main function: X·ª≠ l√Ω ·∫£nh OCR, g·ªôp box, d·ªãch text v√† xu·∫•t JSON
    
    Args:
        image_input: ƒê∆∞·ªùng d·∫´n ·∫£nh ho·∫∑c numpy array
        src_lang: Ng√¥n ng·ªØ ngu·ªìn (m·∫∑c ƒë·ªãnh "ja")
        dest_lang: Ng√¥n ng·ªØ ƒë√≠ch (m·∫∑c ƒë·ªãnh "vi")
        output_json_path: ƒê∆∞·ªùng d·∫´n l∆∞u JSON
        save_debug_image: C√≥ l∆∞u ·∫£nh debug kh√¥ng
        
    Returns:
        dict: K·∫øt qu·∫£ x·ª≠ l√Ω
    """
    # Kh·ªüi t·∫°o OCR
    ocr = PaddleOCR(
        use_doc_orientation_classify=False,
        use_doc_unwarping=False,
        use_textline_orientation=False,
        text_recognition_model_name="PP-OCRv5_server_rec",
        text_detection_model_name="PP-OCRv5_server_det",
    )
    
    # Ki·ªÉm tra input
    if isinstance(image_input, str):
        if not os.path.exists(image_input):
            raise FileNotFoundError(f"·∫¢nh kh√¥ng t·ªìn t·∫°i: {image_input}")
        input_data = image_input
        base_name = os.path.splitext(os.path.basename(image_input))[0]
        image_path = image_input
    elif isinstance(image_input, np.ndarray):
        input_data = image_input
        base_name = "image_array"
        image_path = None
    else:
        raise TypeError("image_input ph·∫£i l√† ƒë∆∞·ªùng d·∫´n ho·∫∑c numpy array")
    
    # T·∫°o th∆∞ m·ª•c output
    output_dir = "./output"
    os.makedirs(output_dir, exist_ok=True)
    
    # Ch·∫°y OCR
    print("üîç ƒêang ch·∫°y OCR...")
    result = ocr.predict(input=input_data)
    
    # Extract text information
    all_texts_with_info = []
    for res in result:
        texts = res.get("rec_texts", [])
        scores = res.get("rec_scores", [])
        boxes = res.get("dt_polys", [])
        
        for text, score, box in zip(texts, scores, boxes):
            text_info = {
                'text': text,
                'confidence': float(score),
                'bbox': box.tolist() if hasattr(box, 'tolist') else box
            }
            all_texts_with_info.append(text_info)
    
    print(f"üìä T√¨m th·∫•y {len(all_texts_with_info)} text boxes ban ƒë·∫ßu")
    
    # T√¨m v√† g·ªôp c√°c box ch·ªìng l√™n nhau
    print("üîÑ ƒêang g·ªôp c√°c box ch·ªìng l√™n nhau...")
    overlapping_groups = find_overlapping_groups(all_texts_with_info)
    merged_texts = merge_overlapping_boxes(all_texts_with_info, overlapping_groups)
    
    # S·∫Øp x·∫øp to√†n b·ªô k·∫øt qu·∫£ theo XY-Cut
    print("üìù ƒêang s·∫Øp x·∫øp text theo th·ª© t·ª± ƒë·ªçc...")
    final_sorted_texts = sort_texts_with_xycut(merged_texts)
    
    # D·ªãch text
    print(f"üåç ƒêang d·ªãch text t·ª´ {src_lang} sang {dest_lang}...")
    for i, text_info in enumerate(final_sorted_texts):
        original_text = text_info['text']
        print(f"ƒêang d·ªãch {i+1}/{len(final_sorted_texts)}: {original_text[:50]}...")
        
        translated_text = translate_text(original_text, src_lang, dest_lang)
        text_info['translated_text'] = translated_text
        text_info['original_text'] = original_text
        text_info['src_lang'] = src_lang
        text_info['dest_lang'] = dest_lang
        
        # time.sleep(0.1)  # Tr√°nh spam API
    
    print("‚úÖ Ho√†n th√†nh d·ªãch text")
    
    # Th·ªëng k√™
    merged_count = sum(1 for group in overlapping_groups if len(group) > 1)
    total_merged_boxes = sum(len(group) for group in overlapping_groups if len(group) > 1)
    
    print(f"üìà K·∫øt qu·∫£ g·ªôp box:")
    print(f"- Box ban ƒë·∫ßu: {len(all_texts_with_info)}")
    print(f"- Nh√≥m g·ªôp: {merged_count}")
    print(f"- Box cu·ªëi c√πng: {len(final_sorted_texts)}")
    
    # Chu·∫©n b·ªã output data
    output_data = {
        'metadata': {
            'image_path': image_path,
            'total_original_boxes': len(all_texts_with_info),
            'total_merged_groups': merged_count,
            'total_final_boxes': len(final_sorted_texts),
            'source_language': src_lang,
            'target_language': dest_lang,
            'processing_timestamp': __import__('datetime').datetime.now().isoformat()
        },
        'texts': []
    }
    
    # Add text data
    for i, text_info in enumerate(final_sorted_texts):
        text_data = {
            'index': i,
            'original_text': text_info['original_text'],
            'translated_text': text_info['translated_text'],
            'confidence': text_info['confidence'],
            'bbox': text_info['bbox'],
            'is_merged': text_info.get('is_merged', False)
        }
        
        if text_info.get('is_merged', False):
            text_data['merged_from_count'] = text_info['merged_from_count']
            text_data['original_texts'] = text_info['original_texts']
        
        output_data['texts'].append(text_data)
    
    # Combined text (c·∫£ g·ªëc v√† d·ªãch)
    output_data['combined_original_text'] = " ".join([t['original_text'] for t in final_sorted_texts])
    output_data['combined_translated_text'] = " ".join([t['translated_text'] for t in final_sorted_texts])
    
    # L∆∞u JSON
    if output_json_path is None:
        output_json_path = os.path.join(output_dir, f"{base_name}_translated_ocr_result.json")
    else:
        output_json_path = os.path.join(output_dir, output_json_path)
    
    with open(output_json_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"üíæ ƒê√£ l∆∞u k·∫øt qu·∫£ JSON v√†o: {output_json_path}")
    
    # L∆∞u ·∫£nh debug n·∫øu c·∫ßn
    if save_debug_image and image_path:
        debug_image_path = os.path.join(output_dir, f"{base_name}_debug_boxes.jpg")
        save_debug_visualization(image_path, final_sorted_texts, debug_image_path)
        print(f"üñºÔ∏è ƒê√£ l∆∞u ·∫£nh debug v√†o: {debug_image_path}")
    
    return output_data

def save_debug_visualization(image_path, text_infos, output_path):
    """L∆∞u ·∫£nh visualization ƒë·ªÉ debug"""
    img = cv2.imread(image_path)
    if img is None:
        return
    
    # V·∫Ω bounding boxes
    for i, text_info in enumerate(text_infos):
        bbox = text_info['bbox']
        
        # Convert bbox to rectangle points
        if len(bbox) == 4 and not isinstance(bbox[0], list):
            # Format [x_min, y_min, x_max, y_max]
            x1, y1, x2, y2 = bbox
            pts = np.array([[x1, y1], [x2, y1], [x2, y2], [x1, y2]], np.int32)
        else:
            # Other formats
            converted = convert_bbox_to_xycut_format(bbox)
            x1, y1, x2, y2 = converted
            pts = np.array([[x1, y1], [x2, y1], [x2, y2], [x1, y2]], np.int32)
        
        # Ch·ªçn m√†u: ƒë·ªè cho merged box, xanh cho single box
        color = (0, 0, 255) if text_info.get('is_merged', False) else (0, 255, 0)
        thickness = 3 if text_info.get('is_merged', False) else 2
        
        cv2.polylines(img, [pts], True, color, thickness)
        
        # V·∫Ω s·ªë th·ª© t·ª±
        cv2.putText(img, str(i), (int(x1), int(y1)-10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
    
    cv2.imwrite(output_path, img)

# Test function
def test_ocr_to_json():
    """Test function ch·ªâ xu·∫•t JSON"""
    image_path = "./cropped_boxes/Chap189-Page011.jpg"  # Thay ƒë·ªïi path n√†y
    
    if os.path.exists(image_path):
        print("=== TEST OCR TO JSON ===")
        
        # Test d·ªãch t·ª´ ti·∫øng Nh·∫≠t sang ti·∫øng Vi·ªát
        print("\nüáØüáµ‚û°Ô∏èüáªüá≥ X·ª≠ l√Ω ·∫£nh: Nh·∫≠t ‚Üí Vi·ªát")
        result = process_image_to_json(
            image_path,
            src_lang="ja",
            dest_lang="vi", 
            output_json_path="translated_ja_to_vi.json",
            save_debug_image=True
        )
        
        print(f"\nüìà K·∫øt qu·∫£:")
        print(f"- Box ban ƒë·∫ßu: {result['metadata']['total_original_boxes']}")
        print(f"- Nh√≥m g·ªôp: {result['metadata']['total_merged_groups']}")
        print(f"- Box cu·ªëi c√πng: {result['metadata']['total_final_boxes']}")
        
        print(f"\nüìù Text g·ªëc (Nh·∫≠t):")
        original_text = result['combined_original_text'][:200] + "..." if len(result['combined_original_text']) > 200 else result['combined_original_text']
        print(original_text)
        
        print(f"\nüìù Text d·ªãch (Vi·ªát):")
        translated_text = result['combined_translated_text'][:200] + "..." if len(result['combined_translated_text']) > 200 else result['combined_translated_text']
        print(translated_text)
        
        print(f"\nüíæ File JSON output: ./output/translated_ja_to_vi.json")
        
        # In chi ti·∫øt v√†i text ƒë·∫ßu ti√™n
        print(f"\nüîç Chi ti·∫øt 3 text ƒë·∫ßu ti√™n:")
        for i, text_data in enumerate(result['texts'][:3]):
            print(f"{i+1}. G·ªëc: '{text_data['original_text']}'")
            print(f"   D·ªãch: '{text_data['translated_text']}'")
            print(f"   Confidence: {text_data['confidence']:.3f}")
            print(f"   Merged: {text_data['is_merged']}")
            print()
        
        return result
        
    else:
        print(f"‚ùå File {image_path} kh√¥ng t·ªìn t·∫°i")
        print("T·∫°o th∆∞ m·ª•c cropped_boxes/ v√† ƒë·∫∑t ·∫£nh test v√†o ƒë√≥")
        return None

def quick_process_to_json(image_path, src_lang="ja", dest_lang="vi"):
    """
    H√†m nhanh ƒë·ªÉ x·ª≠ l√Ω ·∫£nh ra JSON v·ªõi tham s·ªë t·ªëi thi·ªÉu
    
    Args:
        image_path: ƒê∆∞·ªùng d·∫´n ·∫£nh
        src_lang: Ng√¥n ng·ªØ ngu·ªìn
        dest_lang: Ng√¥n ng·ªØ ƒë√≠ch
        
    Returns:
        str: ƒê∆∞·ªùng d·∫´n file JSON
    """
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"·∫¢nh kh√¥ng t·ªìn t·∫°i: {image_path}")
    
    print(f"üöÄ X·ª≠ l√Ω nhanh: {src_lang} ‚Üí {dest_lang}")
    
    result = process_image_to_json(
        image_path,
        src_lang=src_lang,
        dest_lang=dest_lang,
        save_debug_image=False
    )
    
    base_name = os.path.splitext(os.path.basename(image_path))[0]
    json_path = f"./output/{base_name}_translated_ocr_result.json"
    
    print(f"‚úÖ Ho√†n th√†nh! File JSON: {json_path}")
    return json_path

if __name__ == "__main__":
    # Ch·∫°y test c∆° b·∫£n
    test_ocr_to_json()